---
title: "The spectral Galerkin method and Shenfun programming"
subtitle: "11 June 2024, Athens Greece"
author: "Prof. Mikael Mortensen, University of Oslo"
format:
  revealjs: 
    theme: [simple]
jupyter: shenfun
pandoc:
  to: revealjs
  output-file: introduction.html
  standalone: false
  wrap: none
  default-image-extension: png
  html-math-method:
    method: mathjax
    url: >-
      https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
  slide-level: 2
metadata:
  margin: 0.4
---

## A little bit about myself:

- PhD (2005) in turbulent combustion from Chalmers University of Technology, Sweden
- Post Doc (2006) at Sydney University
- Research scientist (2006-2012) at the Norwegian Defence Research Establishment
- Associate Professor at the University of Oslo (2012-2019)
- Full Professor at UiO since 2019

## Main interests

- Computational Fluid Dynamics
- Turbulence and turbulence modelling
- Scientific computing
- High-performance computing
- The spectral Galerkin method

## Outline of talk

- The spectral Galerkin method
- An introduction to Shenfun
- Some Shenfun examples

## The Galerkin method in a nutshell

Assume that you have an equation

$$
\begin{align}
u(x) &= f(x) \\
u'(x) + \alpha u(x) &= f(x) \\
u''(x) &= f(x)
\end{align}
$$

or quite generally with an operator $\mathcal{L}$:

$$
\begin{equation}
\mathcal{L}(u) = f
\end{equation}
$$

for some functions $u(x)$ and $f(x)$ on $x\in[a, b]$. 

The function $u(x)$ is the **solution**.

## Function approximation

The **global** Galerkin method tries to approximate $u(x)$ using 

$$
u(x) \approx u_N(x) = \sum_{i=0}^N \hat{u}_i \psi_i(x)
$$

where $\{\hat{u}_i\}_{i=0}^N$ are unknown coefficients (degrees of freedom).

The functions $\psi_i(x)$ are **basisfunctions**. We say that 

$$\{\psi_i\}_{i=0}^N$$ 

is a **basis** and $\psi_i$ are also called **test** or **trial** functions.

## A simple example

Assume that $f(x) = x^2$ for $x \in [0, 1]$. Try to find the best possible approximation

$$u_N(x) = f(x)$$

when the basis is $\{1, x\}$, i.e., $\psi_0=1$ and $\psi_1=x$.

. . .

In other words, find the unknown $\{\hat{u}_0, \hat{u}_1\}$ such that

$$
u_N(x) = \hat{u}_0 \cdot 1 + \hat{u}_1 \cdot x
$$

is the best possible approximation to $x^2$.

. . .

How is this possible? What options are there? What is **best**?

## What is the best approximation?

. . .

- Two-point Collocation?

. . .

Match endpoints. Set $u_N(x_i)=f(x_i)$ for $x_0=0$ and $x_1=1$
$$u_N(x_i)=\hat{u}_0 + \hat{u}_1 x_i=f(x_i)$$

. . .

We get
$$
\begin{align}
\hat{u}_0 \cdot 1 + \hat{u}_1 \cdot 0 &= 0^2 \\
\hat{u}_0 \cdot 1 + \hat{u}_1 \cdot 1 &= 1^2
\end{align}
$$

Two equations for two unknowns. Solution: $\hat{u}_0=0, \hat{u}_1=1$

$$
u_N(x) = x
$$

## Two-point collocation

```{python}
#| echo: true
#| code-line-numbers: "3-5"
import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 1, 100)
plt.plot(x, x**2, 'r', x, x, 'b')
plt.legend(['Exact', 'Collocation'], fontsize=18)
```

. . .

Is this the best possible approximation? In what sense? How do we pick the collocation points $x_i$ in general?

## What is the best approximation?

. . .

- <span style="color:gray">Two-point Collocation?</span>
- The least squares method?

. . .

Define the error as 

$$
e(x) = f(x)-u_N(x)
$$

Define the $L^2$(a, b) inner product as 

$$
(g, h) = \int_{a}^b g \cdot h \, dx
$$

for some functions $g(x)$ and $h(x)$.

## The least squares method

Define the error $E = (e, e) = |e|^2$. The least squares method requires 

$$
\frac{\partial E}{\partial \hat{u}_i} = 0, \quad i = 0, 1,
$$

which is two equations for our two unknowns.

. . .

Insert for $e=f-u_N$ and use $\frac{\partial u_N}{\partial \hat{u}_i} = \psi_i$:

$$
\sum_{j=0}^1(\psi_j, \psi_i) \hat{u}_j = (f, \psi_i), \quad i=0, 1.
$$

A linear system of equations to solve for vector $\boldsymbol{\hat{u}}=(\hat{u}_0, \hat{u}_1)$ with a mass matrix $a_{ij}=(\psi_j, \psi_i) \in \mathbb{R}^{2 \times 2}$. 

## Least square implementation {visibility="hidden"}

We can implement our testcase easily using Sympy. Assemble matrix $A=(a_{ij}) \in \mathbb{R}^{2 \times 2}$

```{python}
#|echo: true
import sympy as sp
from IPython.display import display
x = sp.symbols('x')
def inner(u, v, domain=(0, 1), x=x):
    return sp.integrate(u*v, (x, domain[0], domain[1]))

A = sp.Matrix(((inner(1, 1), inner(1, x)),
               (inner(x, 1), inner(x, x))))
display(A)
```

## Solve {visibility="hidden"}
Assemble $\boldsymbol{f} = (f, \psi_i)$ for $i=0, 1$
```{python}
#|echo: true
f = x**2
b = sp.Matrix(((inner(f, 1)), (inner(f, x))))
display(b)
```
Solve linear algebra system $A \boldsymbol{\hat{u}}=\boldsymbol{f}$ for $\boldsymbol{\hat{u}}$
```{python}
#|echo: true
u_hat = A.solve(b)
display(u_hat)
```
We get the solution $u_N$:
```{python}
#|echo: true
display(u_hat[0]+u_hat[1]*x)
```
## Least squares method

The least squares method becomes a bit messy for differential equations and is not much used for solving PDEs.

In our case the solution becomes: $u_N(x) = -\frac{1}{6}+x$
```{python}
#| echo: false
#| code-line-numbers: "5"
xj = np.linspace(0, 1, 100)
plt.plot(xj, xj**2, 'r', xj, xj, 'b', xj, -1./6.+xj, 'g')
plt.legend(['Exact', 'Collocation', 'Least squares'], fontsize=18)
```
## What is the best approximation?
. . .

- <span style="color:gray">Two-point Collocation?</span>
- <span style="color:gray">The least squares method?</span>
- The Galerkin method?

. . .

The Galerkin method is similar to the least square method. For approximating functions it is actually identical. However, the approach is much simpler. We simply require that the **error is $L^2$ orthogonal to the test functions** $\psi_i$.

. . .

Find $\hat{u}_0, \hat{u}_1$ such that

$$
(e, \psi_i) = 0, \quad i=0,1.
$$

Again two equations for two unknowns.

## Solve Galerkin

Insert for $e = f - u_N$ to get 

$$
(f - \sum_{j=0}^1 \hat{u}_j \psi_j, \psi_i) = 0
$$

We get the same equation as least squares

$$
\sum_{j=0}^1 (\psi_j, \psi_i) \hat{u}_j = (f, \psi_i), \quad i=0, 1.
$$

and obviously the same solution (but with less hard work!).

## The global Galerkin method in a nutshell

The solution is approximated as
$$
u_N(x) = \sum_{i=0}^N \hat{u}_i \psi_i(x)
$$

. . .

- Choose basis functions that span the entire domain
- Choose basis functions that satisfy the boundary conditions
- Require that the error is orthogonal to the basis functions
- A basis is defined as: $\{\psi_i\}_{i=0}^N$. 
- A function space is defined as:  $V_N = \text{span}\{\psi_i\}_{i=0}^N$


## Galerkin

With the Galerkin method the problem 

$$
\mathcal{L}(u) = f
$$

is declared as follows: Find $u_N \in V_N$ such that

$$
(\mathcal{L}(u_N)-f, v) = 0, \quad \forall v \in V_N
$$

. . .

This is the same as saying: Find $u_N \in V_N$ such that

$$
(\mathcal{L}(u_N)-f, \psi_i) = 0, \quad \forall \, i=0, 1, \ldots, N.
$$

. . .

N+1 equations for N+1 unknowns $\{\hat{u}_i\}_{i=0}^N$.

## What are the best basis functions?

. . .

- sines and cosines (Fourier exponentials)

For example $\psi_j = \sin((j+1)(x+1)\pi), x \in [-1,1]$

```{python}
M = 100
xj = np.linspace(-1, 1, M+1)
sines = np.sin(np.pi/2*(np.arange(6)[None, :]+1)*(xj[:, None]+1))
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
ax1.plot(xj, sines[:, ::2])
ax2.plot(xj, sines[:, 1::2])
ax1.set_title('Even basis functions')
ax2.set_title('Odd basis functions')
ax1.legend([f"$\psi_{i}(x)$" for i in range(0, 6, 2)]);
ax2.legend([f"$\psi_{i}(x)$" for i in range(1, 6, 2)]);
```

## What are the best basis functions?

. . .

- <span style="color:gray">sines and cosines (Fourier exponentials)</span>
- Chebyshev polynomials $\psi_j = T_j(x)$
- Legendre polynomials $\psi_j = L_j(x)$
```{python}
M = 100
xj = np.linspace(-1, 1, M+1)
cheb = np.polynomial.chebyshev.chebvander(xj, 5)
leg = np.polynomial.legendre.legvander(xj, 5)
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
ax1.plot(xj, cheb)
ax2.plot(xj, leg)
ax1.set_title('Chebyshev basis functions', fontsize=16)
ax2.set_title('Legendre basis functions', fontsize=16)
ax1.legend([f"$T_{i}(x)$" for i in range(0, 5)], loc='lower right')
ax2.legend([f"$L_{i}(x)$" for i in range(0, 5)])
```

Also odd/even functions for odd/even indices.

## How about multiple dimensions?

. . .

- Choose one basis for each direction and create tensor product basis functions on Cartesian product grids.

$$
u_N(x, y) = \sum_{i=0}^N \sum_{j=0}^N \hat{u}_{ij} \psi_i(x)\psi_j(y)
$$

Tensor product functionspace

$$
W = V_N \otimes V_N
$$

with basis functions $\{\psi_i(x)\psi_j(y)\}_{i,j=0}^N$.

## Channel flow

- Turbulent channel flow is an ideal case for the spectral Galerkin method.
- Cartesian grid $[-1, 1] \times [0, 2\pi] \times [0, 2\pi]$
- Chebyshev $\times$ Fourier $\times$ Fourier

<img src="https://rawcdn.githack.com/spectralDNS/spectralutilities/473129742f0b5f8d57e8c647809272c0ced99a45/movies/RB_200k_small.png" >

## The spectral Galerkin method

### Advantages {.smaller}

- Makes use of oscillating basis functions with **extremely good approximation preperties**. Requires few degrees of freedom
- Coefficient matrices are often **sparse**
- Coefficient matrices are **robust** (low condition numbers)

### However {.smaller}

- No complex geometries.
- Nonlinear terms implemented explicitly (pseudospectral)
- Generally considered more difficult to implement since we solve equations in spectral space

$$\longrightarrow \text{Shenfun}$$

## Shenfun

- Shenfun aims to automate the implementation of the spectral Galerkin method
- Uses a high-level language, where function spaces and equations are defined and combined 
- Uses MPI to run on multiple processors on supercomputers

<img src="https://rawcdn.githack.com/spectralDNS/spectralutilities/473129742f0b5f8d57e8c647809272c0ced99a45/movies/RB_200k_small.png" style="float:left" width="600">
<img src="https://rawcdn.githack.com/spectralDNS/spectralutilities/473129742f0b5f8d57e8c647809272c0ced99a45/movies/isotropic_cropped.gif" style="float:right" width="400"> 
<p style="clear: both;">